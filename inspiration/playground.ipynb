{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here, we define hyper-parameters, which define how our model learns.\n",
    "Additionally, here could be other global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the decision, whether we use the gpu or cpu. Using the gpu needs torch to be compiled with cuda support! <br>\n",
    "Almost everything needs to be loaded onto the device, by using `.to(device)` on the model or other tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Here we load the data to train our model. Either use a predefined dataset like MNIST or just load your own data.<br>\n",
    "In Detail:\n",
    "- `Dataset` contains the data + labels\n",
    "- `DataLoader` contains method to load data from the Dataset in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "\troot=\"../data\",\n",
    "\ttrain=True,\n",
    "\tdownload=True,\n",
    "\ttransform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "\troot=\"../data\",\n",
    "\ttrain=False,\n",
    "\tdownload=True,\n",
    "\ttransform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "The actual implementation of our network. The only important method is the `forward` method, which defines how our model transforms the input data. The construtor is used to define the used methods, to make our forward method look nice and tidy.\n",
    "\n",
    "Only execute one of the following Cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple feedforward neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "\t\t\tnn.Linear(28*28, 784),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(784, 784),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(784, 10),\n",
    "\t\t\tnn.ReLU()\n",
    "\t\t)\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple UNet\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=64):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Individual convolutions = in_channels * out_channels * 2\n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = self.conv_block(in_channels, out_channels) # 1 -> 64 -> 64\n",
    "        self.down2 = self.conv_block(out_channels, out_channels*2) # 64 -> 128 -> 128\n",
    "        self.down3 = self.conv_block(out_channels*2, out_channels*4) # 128 -> 256 -> 256 \n",
    "        self.down4 = self.conv_block(out_channels*4, out_channels*8) # 256 -> 512 -> 512\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.center = self.conv_block(out_channels*8, out_channels*16) # 512 -> 1024 -> 1024\n",
    "        \n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = self.upconv_block(out_channels*16, out_channels*8) # 1024 -> 512 -> 512 -> 512\n",
    "        self.up2 = self.upconv_block(out_channels*8, out_channels*4) # 512 -> 256 -> 256 -> 256\n",
    "        self.up3 = self.upconv_block(out_channels*4, out_channels*2) # 256 -> 128 -> 128 -> 128\n",
    "        self.up4 = self.upconv_block(out_channels*2, out_channels) # 128 -> 64 -> 64 -> 64\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            # Extract spatial features\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Compact spatial features\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            # Upsample spatial features\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            # Compact spatial features?\n",
    "            self.conv_block(out_channels, out_channels)\n",
    "        )\n",
    "    \n",
    "    # Forward pass - Takes non-flattened images as input (28, 28)\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        # Pooling: Only takes the max value from the 2x2 window. \n",
    "        down1 = self.down1(x) # Input: 1 channel, 28x28 pixels\n",
    "        down2 = self.down2(F.max_pool2d(down1, 2)) # Input (after pooling): 64 channels, 14x14 pixels\n",
    "        down3 = self.down3(F.max_pool2d(down2, 2)) # Input (after pooling): 128 channels, 7x7 pixels\n",
    "        down4 = self.down4(F.max_pool2d(down3, 2)) # Input (after pooling): 256 channels, 3x3 pixels\n",
    "        \n",
    "        # Bottleneck\n",
    "        center = self.center(F.max_pool2d(down4, 2)) # Input (after pooling): 512 channels, 1x1 pixels (!Not checked, could be 2x2)\n",
    "        \n",
    "        # Decoder\n",
    "        up1 = self.up1(center) # Input: 1024 channels, 2x2 pixels (!Not checked, could be 1x1)\n",
    "        up1 = F.interpolate(up1, size=down4.shape[2:])  # Ensure matching dimensions. Makes sure dimension is 3x3 pixels, if not already (rounding error)\n",
    "        up1 = up1 + down4 # Skip connection\n",
    "        \n",
    "        up2 = self.up2(up1) # Input: 512 channels, 3x3 pixels\n",
    "        up2 = F.interpolate(up2, size=down3.shape[2:])  # Ensure matching dimensions. Makes sure dimension is 7x7 pixels, if not already (rounding error)\n",
    "        up2 = up2 + down3 # Skip connection\n",
    "        \n",
    "        up3 = self.up3(up2) # Input: 256 channels, 7x7 pixels\n",
    "        up3 = F.interpolate(up3, size=down2.shape[2:])  # Ensure matching dimensions. Makes sure dimension is 14x14 pixels, if not already (rounding error)\n",
    "        up3 = up3 + down2 # Skip connection\n",
    "        \n",
    "        up4 = self.up4(up3) # Input: 128 channels, 14x14 pixels\n",
    "        up4 = F.interpolate(up4, size=down1.shape[2:])  # Ensure matching dimensions. Makes sure dimension is 28x28 pixels, if not already (rounding error)\n",
    "        up4 = up4 + down1 # Skip connection\n",
    "        \n",
    "        return up4  # Extracted features\n",
    "        \n",
    "# UNet + MLP = CNN (The image segmentation type of CNN)\n",
    "class UNetMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetMLP, self).__init__()\n",
    "\n",
    "        self.unet = UNet(in_channels=1, out_channels=64)  # Extract spatial features\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))   # Pool to (1,1) for compact features???\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 128),  # Hidden layer, why 64???\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),  # Output logits for 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.unet(x)    # Extract features\n",
    "        pooled = self.global_pool(features)  # Global average pooling\n",
    "        logits = self.linear_relu_stack(pooled)  # Classifier\n",
    "        return logits\n",
    "\n",
    "model = UNetMLP().to(device)\n",
    "print(model)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Here we define the learning and evaluation process (during training). The learning steps are as follows:\n",
    "1. Let our model produce a prediction\n",
    "2. Calculate the loss using any loss function\n",
    "3. Calculate the gradient using backpropagation\n",
    "4. Let the optimizer do it's magic (update parameters in some way)\n",
    "5. Restore the gradient of the optimizer to zero, to avoid calculating the new gradient based on the old one\n",
    "\n",
    "The evaluation methods just does the first two steps, and returns how wrong our model is based on the test dataset. Here we wrap our code in torch.no_grad() to avoid calculating unnecessarry gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader: DataLoader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    for ibatch, (input, target) in enumerate(dataloader):\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        pred = model(input)\n",
    "        loss = loss_fn(pred, target)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if ibatch % 100 == 0:\n",
    "            loss, current = loss.item(), ibatch * batch_size + len(input)\n",
    "            # progress bar\n",
    "            width = 50\n",
    "            print(f\"\\r[{'=' * int(width * current / size)}{' ' * (width - int(width * current / size))}] {current:>5d}/{size:>5d} Loss: {loss:>7f}\", end=\"\")\n",
    "    print(f\"\\r[{'=' * width}] Done! Loss: {loss:>7f}\")\n",
    "            \n",
    "def test_loop(dataloader: DataLoader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for input, target in dataloader:\n",
    "            input, target = input.to(device), target.to(device) \n",
    "            pred = model(input)\n",
    "            test_loss += loss_fn(pred, target).item()\n",
    "            correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the train- and eval-loops with the training data for n-epochs. Also define the loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "\tprint(f\"Epoch {t+1}\")\n",
    "\ttrain_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "\ttest_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../models/unet_mlp_mnist_python.pth\")\n",
    "\n",
    "## Load with. But careful: this requires the model class (e.g. UNetMLP) to be defined in the script\n",
    "# model = torch.load(\"models/unet_mlp_mnist_python.pth\").to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Actually do something with the model. For this, the input needs to be exatly as the training data, so in this case we need to convert our image to grayscale, resize it to 28x28, convert it into an tensor and invert the image to make white near to 0 and black near to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(path):\n",
    "    image = Image.open(path).convert(\"L\") # Load image and convert to grayscale\n",
    "    transform = transforms.Compose([\n",
    "\t\ttransforms.Resize((28, 28)),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Lambda(lambda x: 1 - x), # Invert image, so that white is 0 and black is 1\n",
    "\t])\n",
    "    return transform(image).unsqueeze(0) # add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small prediction method, which just takes in the image as a tensor (via loadImage), transfer it onto the `device`, the model is loaded on and infer the result. Then we call softmax (not necessary, because argmax returns the highest value anyway) to normalize our logits into a vector, where all values add up to 1 and range from 0 to 1. Then we take the one with the highest probability of success via `argmax(1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(model, image):\n",
    "\timage = image.to(device)\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tpred = nn.Softmax(dim=1)(model(image))\n",
    "\t\treturn pred.argmax(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the methods..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = loadImage(\"../data/own/5.png\")\n",
    "print(pred(model, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "Here i defined some functions, that output the tensors into a human readable format. I used that, to see if my image was correctly represented as a Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = loadImage(\"5.png\").squeeze().numpy()\n",
    "for row in array:\n",
    "\t\tprint(\" \".join(f\"{pixel:.1f}\" for pixel in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show the first 10 items, e.g. image and label\n",
    "for i in range(10):\n",
    "\timage, label = test_data[i]\n",
    "\timage_array = image.squeeze().numpy()\n",
    "\tprint(label)\n",
    "\tfor row in image_array:\n",
    "\t\tprint(\" \".join(f\"{pixel:.1f}\" for pixel in row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
